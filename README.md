<div align="center">
    <img src="images/logo.png">
</div>
<p align="center">

---

![Static Badge](https://img.shields.io/badge/Repo_Status%3A-Work_in_Progress-blue?style=flat&logo=data%3Aimage%2Fpng%3Bbase64%2CiVBORw0KGgoAAAANSUhEUgAAAC0AAAAiCAMAAAD8kqB9AAAClFBMVEUAAAD%2F%2F%2F%2FBYzTCysr2WxvyQRLnSibjUy3VZSLbQyvSjHHIjEX5%2F%2F%2F5%2B%2FvIz9D8%2F%2F%2F4WhryWRz3QBHxUR7ySh%2FxbBjsTiHwbxnnTCXVQiz96eTg4N%2Fd3dzL0tH0XDPFzc32SRv0UB%2F3WRr4XRn1SR32VBv1Uhz0TB71URz0QBDySh3%2FYAP4ZxT1YxnwXRz0ZxnvTh7vVR7yPhDxQhfrTB%2FcVCLfRSzUUCfdPCbLciPQOjLu8fHc5OXn5eT85uHj4uHY19bKt6%2F3WBv2Uhv0WS%2F1TSDyWjT0Sx33XBr3Vhv0Vxz3XRr3Xhr1Uhv0Ux30SRv0Whv3Yhn2Xhr0UhzzPg%2F2RBb0Tx7zPg%2FzPg7zUR33YxjyTh%2F7XgL6WQDxTh%2FwVB%2F3Pw%2FzPA3wTh7zYRv1aBntTCDxXhzrTx%2FwSh7oYCDxaxjuTiDmSyHsWR7rPxnoQiXnchrmehr%2B9%2FTs7%2FD%2B7unW2djQ19jm2dbHxcLKuLD3v6%2FJrKL1SRvvelv25ePBxML7aRfg7%2FbS5ezY7PDA1t32VBv2UBz0SR70QxT0QxT0QxT2VRv1VRz3Yhn1Sx34VRvyTB%2F1Sh71Rhj2Uhz2TBj3YRn3Yxn1RBb5ZBfxTB%2F2ZBn7Zgz4Zhj4QhPuWB30ZBrySR3yXhzvSyDvWRv3PxDvTR%2F0WRzyYhvyPQ32aRryahrvViDuYhnxQRPzbRrsRh%2FYXSPuchnrcxjRp5vqbEr5v6%2F0cU3wWDD849vQqqD3pI32n4b2moDfhmznlmTocVD0XjX5Vhrl5ubKtKvirZ%2FPpprTpZbWnI%2F8q3z2lnv4lHn5pXXwn27fhGrfgmjlkF%2F5klzmclPjcFD1bUn0Z0Ptd0H3bDb0VCr0WyT4WRn7YRf4PQvBMCeZAAAAs3RSTlMA%2FQj72VQnHBIPBgX%2B%2Fv78%2BKqajGI0LyolBv7%2B%2Fv79%2FPX09PTu287Cv7%2B1oqCTi394c1pONCIYFRINCv7%2B%2Fv7%2B%2Fv79%2Ffv39fHw7evp4uHS0M%2FMysXFwbq4s7Cwq6SimpGQgX59eHJoYFlRUElHRD8sGRX%2B%2Fv7%2B%2Fv7%2B%2Fv7%2B%2Fv38%2FPv6%2Bvn57u3p4%2BLd19bV0tLNzcrIwsC5trSnp6eioJ%2Bem5uVlZWRkIiDbmlcS0tEOTQjHZlkpy4AAAI7SURBVDjLYqAPyBHU0Y6NjZlvCuZxLp%2BtrR2jI5iOS7nWjX2nT1xQ5ARzuKdc3Xf%2BzDV%2Fdlyq2dt3NvHbqUhAzJ50vKWxzi0Pt1sSttRLM25bAWYbbXd0YDwVgcflEirH%2BHi2dpoBmVyhe%2BWtd7isBTLxGM4MNDyRQZKBdXtlKf%2FJCLzBwua%2BGWh4BzsDE8RoY%2FyhuGQLsz3%2F5USg0Y4yjHvCufCrZlPYLMuztYt9%2Bjk%2BXjs5Y0JRJAQ0nHFbiNthaaDRQD4RhlfdPsvHKyXHSjj%2BhfYz2xbVVtsw7tbkIqxaAmg4b3kFwmjChh%2BSgRhNGIi57iyTKS45YIRDHiAONMMvyjvw70Iz2hzOMs3kQDX8qI3UARRXc2duQJLXW4MsN%2FdS864QJiQBUb1cZHnTeQvFETwTxSNXkFxdoCdoguZ2wwDdjXDOguvqcKMLkwOEOTG8mh%2FpoysOM7xGBMrapM8SnIM1aDL8PGat4waxJFPZIe7V9fZNlcQRklYiagKB%2BqLcEJ74yqlKqklmeGLGyjjSU3laimiB%2BCrAoryVwkTMgYGHUzHIUjMDr1sNPso3BeLZcKpjMmfisEwTzErRiQpUVurXmNgm0Dtjjn7WsuRCDqAMWvj1%2BAWxsHgcdPJUnRwnst6CwVIsQ1ij28vpoAALS5Ca6mIU1aujVaINklp9s%2FMtkEQ52fKCneMN4tzDDNGcsmiCsIZzGoYLcxXVhNW1mNCFLZb6q2MrILO1%2BmZKYPMorlBlIBcAACpxj1lvNSqgAAAAAElFTkSuQmCC&labelColor=%23232D4B&color=%23E57200) &nbsp; &nbsp; &nbsp;[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) &nbsp; &nbsp; &nbsp;[![python](https://img.shields.io/badge/Python-3.11-3776AB.svg?style=flat&logo=python&logoColor=white)](https://www.python.org)

# Introduction

Data Scientists and analysts have developed several metrics for determining a player's value to their team's success. Prominent examples include Value Over Replacement Player (VORP), Box Plus/Minus (BPM), and FiveThirtyEight's Robust Algorithm (using) Player Tracking (and) On/Off Ratings (RAPTOR)​. We aim to develop a multivariate index that weighs these parameters based on how well they predict MVP rankings, then test it on unseen data for the most recent five seasons to see if our index correctly predicts the MVP rankings.​ We will experiment with the index formula and compare it to other methods developed by reputable analyst sources.

Click on the Report dropdown menu below to read more about the data, experimental design, results, and conclusions.

<details>
<summary><h1 style="font-size: 22px;">Report</h1></summary>


## Table of Contents

<!--ts-->
   * [Data](#data)
   * [Experimental Design](#experimental-design)
      * [Design Overview](#design-overview)
      * [Feature Selection Process](#feature-selection-process)
      * [Modeling](#modeling)
      * [Index Building](#index-building)
   * [Results](#results)
   * [Testing](#testing)
   * [Conclusions](#conclusions)
<!--te-->

## Data
<a name="data"></a>

We obtained the dataset from [JK-Future](https://github.com/JK-Future-GitHub/NBA_MVP), who originally scraped the data from Basketball-Reference via automated HTML parsing. The dataset contains statistics for National Basketball Association (NBA) players relevant to determining the Most Valuable Player (MVP) in a season and has 7,329 entries with 53 columns. The dataset is significant in its breadth and depth of coverage.

We store the dataset in [mvp_data.csv](https://github.com/WD-Scott/DS5110_Project/blob/main/Data%20Files/mvp_data.csv) and load it into [DataCleaning_EDA.ipynb](https://github.com/UVA-MLSys/Big-Data-Systems/blob/main/Team%207/Jupyter%20Notebooks/DataCleaning_EDA.ipynb), where we perform data cleaning and aggregation.

<details>
<summary><strong>Click here for details about how we cleaned the data</strong></summary>

* Fill missing values for the Rank, mvp_share, and Trp Dbl (Triple Double) columns
* Normalize the Trp Dbl column by dividing it by G (the total number of games played in a given season)
* Convert G (Games) and Season columns to integer data type
* Filter the entire data frame `(df)` to include only players that meet the 40-game requirement necessary to be considered for the MVP award
* Create the Rk_Conf (Conference Ranking) column – calculate conference rankings for each season based on W (the number of wins), then re-rank the conference rankings within each season and conference group
* Save the edited data frame thus far to [mvp_data_edit.csv](https://github.com/UVA-MLSys/Big-Data-Systems/blob/main/Team%207/Data%20Files/mvp_data_edit.csv) (we use this in [Test.ipynb](https://github.com/UVA-MLSys/Big-Data-Systems/blob/main/Team%207/Jupyter%20Notebooks/Test.ipynb) to merge predicted values with actual and compare results)
* Drop the Conference and W (Wins) columns
* Create a separate data frame `(df_last)` with the data for the most recent five seasons (2018–22), which we use to test our final model and index
* Check for missing values: We found many missing values for seasons before 1980; for example, 3P (Three-pointers) were not introduced in the NBA until 1979–80, and there are a lot of missing values before then, so we drop any season before 1980
* Save `df` and `df_last` to comma-separated Excel files
</details>

We discuss additional preprocessing steps in the Experimental Design section below, as these steps relate to the project's feature selection and modeling phases.

The values we seek to predict are in the mvp_share column, which represents the MVP voting result for each season.

## Experimental Design
<a name="experimental-design"></a>

<details>
<summary><strong>Click here for details about our hardware and compute resources</strong></summary>

We use Rivanna – the University of Virginia's High-Performance Computing (HPC) system – with the following hardware details:

- **System**: Linux
- **Release**: 4.18.0-425.10.1.el8_7.x86_64
- **Machine**: x86_64
- **CPU Cores**: 28
- **RAM**: 36GB
- **CPU Vendor**: AuthenticAMD
- **CPU Model**: AMD EPYC 7742 64-Core Processor
</details>

#### Design Overview
<a name="design-overview"></a>

Below is an overview of the steps to gather the index values and model results. We detail these steps further in the Feature Selection Process, Modeling, Results, and Testing sections that follow.

<h1 align="center">
    <img src="https://github.com/WD-Scott/DS5110_Project/blob/main/images/pipeline.png">
</h1>
<p align="center">

#### Feature Selection Process
<a name="feature-selection-process"></a>

In [FeatureSelection.ipynb](https://github.com/UVA-MLSys/Big-Data-Systems/blob/main/Team%207/Jupyter%20Notebooks/FeatureSelection.ipynb), we load in [df_clean.csv](https://github.com/UVA-MLSys/Big-Data-Systems/blob/main/Team%207/Data%20Files/df_clean.csv) as a Pandas DataFrame `(df)` and perform robust feature selection using the `preprocess_and_train` function from [preptrain.py](https://github.com/UVA-MLSys/Big-Data-Systems/blob/main/Team%207/Python%20Modules/preptrain.py). The `preprocess_and_train` function serves to:

* Impute missing values with the median value for numeric features, scale the features using standardization (subtracting the mean and dividing by the standard deviation) and apply one-hot encoding for categorical features.

* Apply the preprocessing separately to the training and testing datasets and extract the feature names, removing any prefixes.

* Train and test eight different models on the preprocessed data and extract the feature importance scores of the top ten predictors. The models are:

  - Random Forest (RF)
  - Decision Tree (DTree)
  - Principal Component Analysis (PCA)
  - Gradient Boosting (GB)
  - Support Vector (SVR)
  - Extra Trees (XTrees)
  - AdaBoost (Ada)
  - Extreme Gradient Boosting (XGB)

For hyperparameter tuning, we define a reasonably extensive parameter grid for each method and use Bayesian optimization with five-fold cross-validation to sample parameter settings from the specified distributions.

We set the `n_jobs` parameter to $-1$ in the [BayesSearchCV](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html) initialization, instructing `scikit-learn` to use all available CPU cores during cross-validation. Thus, each fold's training and evaluation are executed concurrently on different CPU cores, reducing the overall time taken for cross-validation. This parallelization strategy helps to decrease the overall time required for cross-validation, which is particularly beneficial for speeding up the hyperparameter search process.

After running the `preprocess_and_train` function, we use the `print_dict_imps` function from [helper_functions.py](https://github.com/UVA-MLSys/Big-Data-Systems/blob/main/Team%207/Python%20Modules/helper_functions.py) to print tables of the feature importances for each method, which the `preprocess_and_train` function stores in a Python dictionary. We then use the `avg_imp` function from [helper_functions.py](https://github.com/UVA-MLSys/Big-Data-Systems/blob/main/Team%207/Python%20Modules/helper_functions.py) to display the average feature importance across the eight methods. 

The results for the top 10 features included several highly correlated features related to points (scoring), including FT (free throws), 2P (two-pointers), FG (field goals), FGA (field goal attempts), FTA (free throw attempts), and PTS (points).

We chose to drop all of these except PTS because the latter effectively captures the others. The resulting top ten features are:

- WS/48 = Win Shares per 48
- MP = Minutes Played
- PTS = Points
- WS = Win Shares (see <a href="https://www.basketball-reference.com/about/ws.html">NBA Win Shares</a>)
- VORP = Value Over Replacement Player
- PER = Player Efficiency Rating (see <a href="https://www.basketball-reference.com/about/per.html">Calculating PER</a>)
- eFG% = Effective Field Goal Percentage
- AST = Assists
- Rk_Year = Team Ranking
- DBPM = Defensive Box Plus-Minus

There are still some highly correlated features, but we proceed with these ten and save them to [df_selected.csv](https://github.com/UVA-MLSys/Big-Data-Systems/blob/main/Team%207/Data%20Files/df_selected.csv) to use for modeling.

#### Modeling
<a name="modeling"></a>

In [Models.ipynb](https://github.com/UVA-MLSys/Big-Data-Systems/blob/main/Team%207/Jupyter%20Notebooks/Models.ipynb), we use the `train_models` function from [modeling.py](https://github.com/UVA-MLSys/Big-Data-Systems/blob/main/Team%207/Python%20Modules/modeling.py) to train and test only the ensemble and tree-based methods, as these are best suited for our next task — finding the best model we can and using the feature importance scores to inform our index design.

In [Test.ipynb](https://github.com/UVA-MLSys/Big-Data-Systems/blob/main/Team%207/Jupyter%20Notebooks/Test.ipynb), we load in the selected features, the training dataset, the testing dataset containing the data for the 2018–22 seasons, and the best model from [Models.ipynb](https://github.com/UVA-MLSys/Big-Data-Systems/blob/main/Team%207/Jupyter%20Notebooks/Models.ipynb). We filter the training and testing data to include only the selected features.

We then perform an 80-20 train/test split of the training data and test the best model. Next, we use the best model to predict the mvp_share for the 2018–22 seasons and compare the predicted values to the actual values.

The Results section below discuss the results from our feature selection and modeling processes, and the Testing section contains results from testing our best model and index.

#### Index Building
<a name="index-building"></a>

TBD...

### Results
<a name="results"></a>

The feature selection process originally produced a set of ten highly correlated features, the most correlated of which are related to scoring, as displayed below in the correlation heatmap:

<h1 align="center">
    <img src="https://github.com/WD-Scott/DS5110_Project/blob/main/images/corr_scoring.png">
</h1>
<p align="center">

As mentioned, we dropped FT, 2P, FG, FGA, and FTA but retained PTS. Now, the top ten features include those displayed in the correlation heatmap below:

<h1 align="center">
    <img src="https://github.com/WD-Scott/DS5110_Project/blob/main/images/corr_final.png">
</h1>
<p align="center">

We feed these ten features into the `train_models` function, which returns several key pieces of information, including the best model. The `train_models` function also displays neat tables of the feature importance values from each model and a model performance bar chart, as displayed below:

<h1 align="center">
    <img src="https://github.com/WD-Scott/DS5110_Project/blob/main/images/model_comp.png">
</h1>
<p align="center">

The chart shows clearly that the best model is the Extreme Gradient Boosting Regressor (XGB), and the `train_models` function saves the best model to `best_model.pkl` using the `joblib` library.

We import the best model into [Test.ipynb](https://github.com/WD-Scott/DS5110_Project/blob/main/Jupyter%20Notebooks/Test.ipynb) to perform testing on the unseen data.

The chart below displays the predicted values from the best model compared to the actual values:

<h1 align="center">
    <img src="https://github.com/WD-Scott/DS5110_Project/blob/main/images/pred_act.png">
</h1>
<p align="center">

### Testing
<a name="testing"></a>

TBD ...


### Conclusions
<a name="conclusions"></a>

TBD ...

</details>

<details>
<summary><h1 style="font-size: 16px;">Manifest</h1></summary>

<details>
<summary><h3 style="font-size: 14px;">Jupyter Notebooks</h3></summary>
  
- #### [FeatureSelection.ipynb](https://github.com/WD-Scott/DS5110_Project/blob/main/Jupyter%20Notebooks/FeatureSelection.ipynb):

  Feature Selection notebook where we use the `preprocess_and_train` function from `preptrain.py` and ensemble the methods to generate the best 10 features.
  
- #### [DataCleaning_EDA.ipynb](https://github.com/WD-Scott/DS5110_Project/blob/main/Jupyter%20Notebooks/DataCleaning_EDA.ipynb):
  
  Exploratory notebook where the data is cleaned; includes some basic EDA.

- #### [Models.ipynb](https://github.com/WD-Scott/DS5110_Project/blob/main/Jupyter%20Notebooks/Models.ipynb):

  Modeling notebook where we use the selected features (from `df_selected.csv`) to train and evaluate a range of models and extract their feature importance. These results will inform how we weight features in the index.

- #### [Test.ipynb](https://github.com/WD-Scott/DS5110_Project/blob/main/Jupyter%20Notebooks/Test.ipynb):

  This notebook contains the code where we test our best model (from `Models.ipynb`) against the last five seasons. We include some visualizations showing the model prediction versus the actual values.

</details>

<details>
<summary><h3 style="font-size: 14px;">Data Files</h3></summary>
  
- #### [df_clean.csv](https://github.com/WD-Scott/DS5110_Project/blob/main/Data%20Files/df_clean.csv):
  
  Main file used for training and validation.

- #### [df_last.csv](https://github.com/WD-Scott/DS5110_Project/blob/main/Data%20Files/df_last.csv):
  
  Testing file for examining model performance on last 5 seasons (2018-22).

- #### [df_selected.csv](https://github.com/WD-Scott/DS5110_Project/blob/main/Data%20Files/df_selected.csv):

  Selected features containing the subset of predictor variables.

- #### [mvp_data.csv](https://github.com/WD-Scott/DS5110_Project/blob/main/Data%20Files/mvp_data.csv):

  Initial NBA mvp data set. Reduced in [DataCleaning_EDA.ipynb](https://github.com/WD-Scott/DS5110_Project/blob/main/Jupyter%20Notebooks/DataCleaning_EDA.ipynb) to only include essential rows and columns of study.

- #### [mvp_data_edit.csv](https://github.com/WD-Scott/DS5110_Project/blob/main/Data%20Files/mvp_data_edit.csv)

  The cleaned data from [DataCleaning_EDA.ipynb](https://github.com/WD-Scott/DS5110_Project/blob/main/Jupyter%20Notebooks/DataCleaning_EDA.ipynb), used in [Test.ipynb](https://github.com/WD-Scott/DS5110_Project/blob/main/Jupyter%20Notebooks/Test.ipynb) to merge and compare predicted and actual values.

- #### [results.csv](https://github.com/WD-Scott/DS5110_Project/blob/main/Data%20Files/results.csv)

  The full dataset with the index values calculated and stored as an additional column.
  
</details>

<details>
<summary><h3 style="font-size: 14px;">Python Modules (helper functions, classes)</h3></summary>
  
- #### [preptrain.py](https://github.com/WD-Scott/DS5110_Project/blob/main/Python%20Modules/preptrain.py):
  
  Custom function/pipeline for preprocessing and feature selection.

- #### [modeling.py](https://github.com/WD-Scott/DS5110_Project/blob/main/Python%20Modules/modeling.py):

  Custom function/pipeline to train the ensemble and tree-based models and extract the best model.

- #### [helper_functions.py](https://github.com/WD-Scott/DS5110_Project/blob/main/Python%20Modules/helper_functions.py):

This module contains various helper functions for system information retrieval, model evaluation, and visualization.
    
- `get_hardware_details()`:
  
  Retrieve basic hardware details of the system.

- `print_importances(features, model)`:
  
  Print the feature importances of a model.

- `print_dict_imps(feature_importances)`:
  
  Print the feature importances in a visually appealing table format side-by-side.

- `avg_imps(feature_importances)`:
  
  Calculate the average feature importances across different methods.

- `create_imp_df(model_names, models, feature_names)`:
  
  Create a DataFrame of feature importances for each model.

- `plot_corr_heatmap(corr_matrix, selected_feature_names, threshold=0.65, width=7, height=4)`:
  
  Plot a correlation heatmap for selected features.

- `plot_model_performance(model_names, r_sqs, MSE_s)`:
  
  Plot the R-squared and MSE values of different regression models.

- `plot_comparison_for_season(df, season)`

  Plot the actual vs. predicted mvp_share values.
  
</details>

<details>
<summary><h3 style="font-size: 14px;">Other Files</h3></summary>

- #### [images](https://github.com/WD-Scott/DS5110_Project/tree/main/images):

The images folder contains various visualizations and images used in the README.md

- #### [README.md](https://github.com/WD-Scott/DS5110_Project/blob/main/README.md):

The README.md file includes the repository description and the report.

- #### [requirements.txt](https://github.com/WD-Scott/DS5110_Project/blob/main/requirements.txt):

This file includes all of the necessary libraries and versions for running our code.
</details>
</details>
