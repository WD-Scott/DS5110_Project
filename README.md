<div align="center">
    <img src="images/logo.png">
</div>
<p align="center">

---

![Static Badge](https://img.shields.io/badge/Repo_Status%3A-Work_in_Progress-blue?style=flat&logo=data%3Aimage%2Fpng%3Bbase64%2CiVBORw0KGgoAAAANSUhEUgAAAC0AAAAiCAMAAAD8kqB9AAAClFBMVEUAAAD%2F%2F%2F%2FBYzTCysr2WxvyQRLnSibjUy3VZSLbQyvSjHHIjEX5%2F%2F%2F5%2B%2FvIz9D8%2F%2F%2F4WhryWRz3QBHxUR7ySh%2FxbBjsTiHwbxnnTCXVQiz96eTg4N%2Fd3dzL0tH0XDPFzc32SRv0UB%2F3WRr4XRn1SR32VBv1Uhz0TB71URz0QBDySh3%2FYAP4ZxT1YxnwXRz0ZxnvTh7vVR7yPhDxQhfrTB%2FcVCLfRSzUUCfdPCbLciPQOjLu8fHc5OXn5eT85uHj4uHY19bKt6%2F3WBv2Uhv0WS%2F1TSDyWjT0Sx33XBr3Vhv0Vxz3XRr3Xhr1Uhv0Ux30SRv0Whv3Yhn2Xhr0UhzzPg%2F2RBb0Tx7zPg%2FzPg7zUR33YxjyTh%2F7XgL6WQDxTh%2FwVB%2F3Pw%2FzPA3wTh7zYRv1aBntTCDxXhzrTx%2FwSh7oYCDxaxjuTiDmSyHsWR7rPxnoQiXnchrmehr%2B9%2FTs7%2FD%2B7unW2djQ19jm2dbHxcLKuLD3v6%2FJrKL1SRvvelv25ePBxML7aRfg7%2FbS5ezY7PDA1t32VBv2UBz0SR70QxT0QxT0QxT2VRv1VRz3Yhn1Sx34VRvyTB%2F1Sh71Rhj2Uhz2TBj3YRn3Yxn1RBb5ZBfxTB%2F2ZBn7Zgz4Zhj4QhPuWB30ZBrySR3yXhzvSyDvWRv3PxDvTR%2F0WRzyYhvyPQ32aRryahrvViDuYhnxQRPzbRrsRh%2FYXSPuchnrcxjRp5vqbEr5v6%2F0cU3wWDD849vQqqD3pI32n4b2moDfhmznlmTocVD0XjX5Vhrl5ubKtKvirZ%2FPpprTpZbWnI%2F8q3z2lnv4lHn5pXXwn27fhGrfgmjlkF%2F5klzmclPjcFD1bUn0Z0Ptd0H3bDb0VCr0WyT4WRn7YRf4PQvBMCeZAAAAs3RSTlMA%2FQj72VQnHBIPBgX%2B%2Fv78%2BKqajGI0LyolBv7%2B%2Fv79%2FPX09PTu287Cv7%2B1oqCTi394c1pONCIYFRINCv7%2B%2Fv7%2B%2Fv79%2Ffv39fHw7evp4uHS0M%2FMysXFwbq4s7Cwq6SimpGQgX59eHJoYFlRUElHRD8sGRX%2B%2Fv7%2B%2Fv7%2B%2Fv7%2B%2Fv38%2FPv6%2Bvn57u3p4%2BLd19bV0tLNzcrIwsC5trSnp6eioJ%2Bem5uVlZWRkIiDbmlcS0tEOTQjHZlkpy4AAAI7SURBVDjLYqAPyBHU0Y6NjZlvCuZxLp%2BtrR2jI5iOS7nWjX2nT1xQ5ARzuKdc3Xf%2BzDV%2Fdlyq2dt3NvHbqUhAzJ50vKWxzi0Pt1sSttRLM25bAWYbbXd0YDwVgcflEirH%2BHi2dpoBmVyhe%2BWtd7isBTLxGM4MNDyRQZKBdXtlKf%2FJCLzBwua%2BGWh4BzsDE8RoY%2FyhuGQLsz3%2F5USg0Y4yjHvCufCrZlPYLMuztYt9%2Bjk%2BXjs5Y0JRJAQ0nHFbiNthaaDRQD4RhlfdPsvHKyXHSjj%2BhfYz2xbVVtsw7tbkIqxaAmg4b3kFwmjChh%2BSgRhNGIi57iyTKS45YIRDHiAONMMvyjvw70Iz2hzOMs3kQDX8qI3UARRXc2duQJLXW4MsN%2FdS864QJiQBUb1cZHnTeQvFETwTxSNXkFxdoCdoguZ2wwDdjXDOguvqcKMLkwOEOTG8mh%2FpoysOM7xGBMrapM8SnIM1aDL8PGat4waxJFPZIe7V9fZNlcQRklYiagKB%2BqLcEJ74yqlKqklmeGLGyjjSU3laimiB%2BCrAoryVwkTMgYGHUzHIUjMDr1sNPso3BeLZcKpjMmfisEwTzErRiQpUVurXmNgm0Dtjjn7WsuRCDqAMWvj1%2BAWxsHgcdPJUnRwnst6CwVIsQ1ij28vpoAALS5Ca6mIU1aujVaINklp9s%2FMtkEQ52fKCneMN4tzDDNGcsmiCsIZzGoYLcxXVhNW1mNCFLZb6q2MrILO1%2BmZKYPMorlBlIBcAACpxj1lvNSqgAAAAAElFTkSuQmCC&labelColor=%23232D4B&color=%23E57200) &nbsp; &nbsp; &nbsp;[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) &nbsp; &nbsp; &nbsp;[![python](https://img.shields.io/badge/Python-3.11-3776AB.svg?style=flat&logo=python&logoColor=white)](https://www.python.org)

# Introduction

Data Scientists and analysts have developed several metrics for determining a player's value to their team's success. Prominent examples include Value Over Replacement Player (VORP), Box Plus/Minus (BPM), and FiveThirtyEight's Robust Algorithm (using) Player Tracking (and) On/Off Ratings (RAPTOR)​. We aim to develop a multivariate index that weighs these parameters based on how well they predict MVP rankings, then test it on unseen data for the most recent five seasons to see if our index correctly predicts the MVP rankings.​ We will experiment with the index formula and compare it to other methods developed by reputable analyst sources.

Click on the Report dropdown menu below to read more about the data, experimental design, results, testing, and conclusions.

<details>
<summary><h1 style="font-size: 22px;">Report</h1></summary>


## Table of Contents

<!--ts-->
   * [Data](#data)
   * [Experimental Design](#experimental-design)
      * [Design Overview](#design-overview)
      * [Feature Selection Process](#feature-selection-process)
      * [Modeling](#modeling)
      * [Index Building](#index-building)
   * [Results](#results)
   * [Testing](#testing)
   * [Conclusions](#conclusions)
<!--te-->

## Data
<a name="data"></a>

We obtained the dataset from [JK-Future](https://github.com/JK-Future-GitHub/NBA_MVP), who originally scraped the data from Basketball-Reference via automated HTML parsing. The dataset contains statistics for National Basketball Association (NBA) players relevant to determining the Most Valuable Player (MVP) in a season and has 7,329 entries with 53 columns. The dataset is significant in its breadth and depth of coverage.

We store the dataset in [mvp_data.csv](https://github.com/WD-Scott/DS5110_Project/blob/main/Data%20Files/mvp_data.csv) and load it into [DataCleaning_EDA.ipynb](https://github.com/UVA-MLSys/Big-Data-Systems/blob/main/Team%207/Jupyter%20Notebooks/DataCleaning_EDA.ipynb), where we perform data cleaning and aggregation.

<details>
<summary><strong>Click here for details about how we cleaned the data</strong></summary>

* Fill missing values for the Rank, mvp_share, and Trp Dbl (Triple Double) columns
* Normalize the Trp Dbl column by dividing it by G (the total number of games played in a given season)
* Convert G (Games) and Season columns to integer data type
* Filter the entire data frame `(df)` to include only players that meet the 40-game requirement necessary to be considered for the MVP award
* Create the Rk_Conf (Conference Ranking) column – calculate conference rankings for each season based on W (the number of wins), then re-rank the conference rankings within each season and conference group
* Save the edited data frame thus far to [mvp_data_edit.csv](https://github.com/UVA-MLSys/Big-Data-Systems/blob/main/Team%207/Data%20Files/mvp_data_edit.csv) (we use this in [Test.ipynb](https://github.com/UVA-MLSys/Big-Data-Systems/blob/main/Team%207/Jupyter%20Notebooks/Test.ipynb) to merge predicted values with actual and compare results)
* Drop the Conference and W (Wins) columns
* Create a separate data frame `(df_last)` with the data for the most recent five seasons (2018–22), which we use to test our final model and index
* Check for missing values: We found many missing values for seasons before 1980; for example, 3P (Three-pointers) were not introduced in the NBA until 1979–80, and there are a lot of missing values before then, so we drop any season before 1980
* Save `df` and `df_last` to comma-separated Excel files
</details>

We discuss additional preprocessing steps in the Experimental Design section below, as these steps relate to the project's feature selection and modeling phases.

The values we seek to predict are in the mvp_share column, which represents the MVP voting result for each season.

## Experimental Design
<a name="experimental-design"></a>

<details>
<summary><strong>Click here for details about our hardware and compute resources</strong></summary>

We use Rivanna – the University of Virginia's High-Performance Computing (HPC) system – with the following hardware details:

- **System**: Linux
- **Release**: 4.18.0-425.10.1.el8_7.x86_64
- **Machine**: x86_64
- **CPU Cores**: 28
- **RAM**: 36GB
- **CPU Vendor**: AuthenticAMD
- **CPU Model**: AMD EPYC 7742 64-Core Processor
</details>

#### Design Overview
<a name="design-overview"></a>

Below is an overview of the steps to gather the index values and model results. We detail these steps further in the Feature Selection Process, Modeling, Results, and Testing sections that follow.

<h1 align="center">
    <img src="https://github.com/WD-Scott/DS5110_Project/blob/main/images/pipeline.png">
</h1>
<p align="center">

#### Feature Selection Process
<a name="feature-selection-process"></a>

In [FeatureSelection.ipynb](https://github.com/UVA-MLSys/Big-Data-Systems/blob/main/Team%207/Jupyter%20Notebooks/FeatureSelection.ipynb), we load in [df_clean.csv](https://github.com/UVA-MLSys/Big-Data-Systems/blob/main/Team%207/Data%20Files/df_clean.csv) as a Pandas DataFrame `(df)` and perform robust feature selection using the `preprocess_and_train` function from [preptrain.py](https://github.com/UVA-MLSys/Big-Data-Systems/blob/main/Team%207/Python%20Modules/preptrain.py). The `preprocess_and_train` function serves to:

* Impute missing values with the median value for numeric features, scale the features using standardization (subtracting the mean and dividing by the standard deviation) and apply one-hot encoding for categorical features.

* Apply the preprocessing separately to the training and testing datasets and extract the feature names, removing any prefixes.

* Train and test eight different models on the preprocessed data and extract the feature importance scores of the top ten predictors. The models are:

  - Random Forest (RF)
  - Decision Tree (DTree)
  - Principal Component Analysis (PCA)
  - Gradient Boosting (GB)
  - Support Vector (SVR)
  - Extra Trees (XTrees)
  - AdaBoost (Ada)
  - Extreme Gradient Boosting (XGB)

For hyperparameter tuning, we define a reasonably extensive parameter grid for each method and use Bayesian optimization with five-fold cross-validation to sample parameter settings from the specified distributions.

We set the `n_jobs` parameter to $-1$ in the [BayesSearchCV](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html) initialization, instructing `scikit-learn` to use all available CPU cores during cross-validation. Thus, each fold's training and evaluation are executed concurrently on different CPU cores, reducing the overall time taken for cross-validation. This parallelization strategy helps to decrease the overall time required for cross-validation, which is particularly beneficial for speeding up the hyperparameter search process.

After running the `preprocess_and_train` function, we use the `print_dict_imps` function from [helper_functions.py](https://github.com/UVA-MLSys/Big-Data-Systems/blob/main/Team%207/Python%20Modules/helper_functions.py) to print tables of the feature importances for each method, which the `preprocess_and_train` function stores in a Python dictionary. We then use the `avg_imp` function from [helper_functions.py](https://github.com/UVA-MLSys/Big-Data-Systems/blob/main/Team%207/Python%20Modules/helper_functions.py) to display the average feature importance across the eight methods. 

The results for the top 10 features included several highly correlated features related to points (scoring), including FT (free throws), 2P (two-pointers), FG (field goals), FGA (field goal attempts), FTA (free throw attempts), and PTS (points).

We chose to drop all of these except PTS because the latter effectively captures the others. The resulting top ten features are:

- WS/48 = Win Shares per 48
- MP = Minutes Played
- PTS = Points
- WS = Win Shares (see <a href="https://www.basketball-reference.com/about/ws.html">NBA Win Shares</a>)
- VORP = Value Over Replacement Player
- PER = Player Efficiency Rating (see <a href="https://www.basketball-reference.com/about/per.html">Calculating PER</a>)
- eFG% = Effective Field Goal Percentage
- AST = Assists
- Rk_Year = Team Ranking
- DBPM = Defensive Box Plus-Minus

There are still some highly correlated features, but we proceed with these ten and save them to [df_selected.csv](https://github.com/UVA-MLSys/Big-Data-Systems/blob/main/Team%207/Data%20Files/df_selected.csv) to use for modeling.

#### Modeling
<a name="modeling"></a>

In [Models.ipynb](https://github.com/UVA-MLSys/Big-Data-Systems/blob/main/Team%207/Jupyter%20Notebooks/Models.ipynb), we use the `train_models` function from [modeling.py](https://github.com/UVA-MLSys/Big-Data-Systems/blob/main/Team%207/Python%20Modules/modeling.py) to train and test only the ensemble and tree-based methods, as these are best suited for our next task — finding the best model we can and using the feature importance scores to inform our index design.

In [Test.ipynb](https://github.com/UVA-MLSys/Big-Data-Systems/blob/main/Team%207/Jupyter%20Notebooks/Test.ipynb), we load in the selected features, the training dataset, the testing dataset containing the data for the 2018–22 seasons, and the best model from [Models.ipynb](https://github.com/UVA-MLSys/Big-Data-Systems/blob/main/Team%207/Jupyter%20Notebooks/Models.ipynb). We filter the training and testing data to include only the selected features.

We then perform an 80-20 train/test split of the training data and test the best model. Next, we use the best model to predict the mvp_share for the 2018–22 seasons and compare the predicted values to the actual values.

The Results section below discuss the results from our feature selection and modeling processes, and the Testing section contains results from testing our best model and index.

#### Index Building
<a name="index-building"></a>

TBD...

### Results
<a name="results"></a>

The feature selection process originally produced a set of ten highly correlated features, the most correlated of which are related to scoring, as displayed below in the correlation heatmap:

<h1 align="center">
    <img src="https://github.com/WD-Scott/DS5110_Project/blob/main/images/corr_scoring.png">
</h1>
<p align="center">

As mentioned, we dropped FT, 2P, FG, FGA, and FTA but retained PTS. Now, the top ten features include those displayed in the correlation heatmap below:

<h1 align="center">
    <img src="https://github.com/WD-Scott/DS5110_Project/blob/main/images/corr_final.png">
</h1>
<p align="center">

We feed these ten features into the `train_models` function, which returns several key pieces of information, including the best model. The `train_models` function also displays neat tables of the feature importance values from each model and a model performance bar chart, as displayed below:

<h1 align="center">
    <img src="https://github.com/WD-Scott/DS5110_Project/blob/main/images/model_comp.png">
</h1>
<p align="center">

The chart shows clearly that the best model is the Extreme Gradient Boosting Regressor (XGB), and the `train_models` function saves the best model to `best_model.pkl` using the `joblib` library.

We import the best model into [Test.ipynb](https://github.com/WD-Scott/DS5110_Project/blob/main/Jupyter%20Notebooks/Test.ipynb) to perform testing on the unseen data.

The chart below displays the predicted values from the best model compared to the actual values; the model orange markers are the predicted value, and the dark blue markers are the actual value:

<h1 align="center">
    <img src="https://github.com/WD-Scott/DS5110_Project/blob/main/images/pred_act.png">
</h1>
<p align="center">

The range plot shows that the predicted values for mvp_share are, at least for these top four candidates for the 2018–22 seasons, pretty far off. There are some player-year combinations (Damian Lillard, 2018; Nikola Jokic, 2019; and James Harden, 2020) for which the predicted value is very close to the actual.

The table below shows whether the model correctly predicted the top four rankings for the 2018–22 seasons; the model accurately predicts which players are in the top four each season but doesn't always order them correctly. 

<h1 align="center">
    <img src="https://github.com/WD-Scott/DS5110_Project/blob/main/images/table_ranks.png">
</h1>
<p align="center">

The predictions for the 2018 season were perfect in terms of ranking, but the model's rankings for the next four seasons are slightly off. The rankings for 1st and 2nd for the 2019 season are correct, but the model swaps the 3rd and 4th place candidates. For the 2020 season, the model correctly ranks the 1st and 4th place candidates but swaps 2nd and 3rd place. The model correctly ranks the 1st and 3rd place candidates for the 2021 season but places 2nd and 4th out of order. For the 2022 season, the model incorrectly ranks the 1st and 3rd place candidates but correctly ranks 2nd and 4th.
      
### Testing
<a name="testing"></a>

TBD ...


### Conclusions
<a name="conclusions"></a>

TBD ...

</details>

<details>
<summary><h1 style="font-size: 16px;">Minimal Reproducible Code</h1></summary>

Each dropdown menu below contains the minimal reproducible code for reach of the Jupyter Notebooks:

<details>
<summary><h2 style="font-size: 14px;">DataCleaning_EDA</h2></summary>
    
```python
import pandas as pd
import numpy as np
import os
os.chdir('...')

df = pd.read_csv('mvp_data.csv')

# Fill missing values
df['Rank'].fillna(0, inplace=True)
df['mvp_share'].fillna(0.0, inplace=True)
df['Trp Dbl'].fillna(0, inplace=True)

# Normalize Triple Double
df['Trp Dbl'] = df['Trp Dbl'] / df['G']

# Convert 'G' and 'Season' to integer type
df['G'] = df['G'].astype(int)
df['Season'] = df['Season'].astype(int)

# Filter out data based on conditions
df = df[(df['G'] > 40) & (df['Season'] <= 2022)]

# Ranking Conference
df['Rk_Conf'] = df.groupby(['Season', 'conference'])['W'].rank("dense", ascending=False) + df['Rk_Year']
df['Rk_Conf'] = df.groupby(['Season', 'conference'])['Rk_Conf'].rank("dense", ascending=True)

# Create df_full
df.to_csv("mvp_data_edit.csv", index=False, encoding="utf-8-sig")

# Drop Wins and Conference
df.drop(columns=['conference', 'W'], inplace=True)

# Sort out seasons we'll use for testing/predictions
df.sort_values(by=['Season'], ascending=False, inplace=True)
df_last = df[df['Season'] > (2022 - 5)] 

# Filter for seasons older than 5 years
df = df[df['Season'] <= (2022 - 5)]
df.drop(columns=['name'], inplace=True)

df = df[df['Season'] >= 1980]
df.drop(['Season'], axis="columns", inplace=True)

df.to_csv('df_clean.csv', index=False)
df_last.to_csv('df_last.csv', index=False)
```

</details>

<details>
<summary><h2 style="font-size: 14px;">FeatureSelection</h2></summary>

```python
import pandas as pd
import numpy as np
import time

import os
os.chdir('...')
from preptrain import preprocess_and_train
from helper_functions import print_importances, print_dict_imps4x2, avg_imps, plot_corr_heatmap

# Load the data
df = pd.read_csv('df_clean.csv')
df_last = pd.read_csv('df_last.csv')
labels = df.pop("mvp_share")
stratify = df.pop("Rank")

start_time = time.time()

# Call the function to preprocess the data and perform feature selection
(features_rf,
 features_Dtree,
 features_pca, 
 features_gbm,
 features_svr, 
 features_Xtrees,
 features_Ada,
 features_XGB,
 feature_importances) = preprocess_and_train(df, df_last, labels)

end_time = time.time()
execution_time = end_time - start_time
print(f"Feature Selection execution time: {round(execution_time/60, 2)} minutes")

selected_features = ['WS/48', 'MP', 'PTS', 'WS', 'VORP', 'PER', 'eFG%', 'AST', 'Rk_Year', 'DBPM']

df_selected = df[selected_features]
df_selected.to_csv('df_selected.csv', index=False)
```
</details>

<details>
<summary><h2 style="font-size: 14px;">Models</h2></summary>

```python
import time
import os
os.chdir('...')
from modeling import train_models
from helper_functions import get_hardware_details

df = pd.read_csv('df_clean.csv')
labels = df.pop("mvp_share")
df_selected = pd.read_csv('df_selected.csv')
feature_names = list(df_selected.columns)

start_time = time.time()

trained_models, results, best_model_name, best_model = train_models(df_selected,
                                                                    df,
                                                                    labels,
                                                                    feature_names,
                                                                    label_col_name="mvp_share")

end_time = time.time()
execution_time = end_time - start_time
print(f"Model building execution time: {round(execution_time/60, 2)} minutes")
```
</details>

<details>
<summary><h2 style="font-size: 14px;">Test</h2></summary>

```python
import os
os.chdir('/sfs/qumulo/qhome/bdr6qz/Documents/MSDS/DS6050')
from helper_functions import print_importances, print_dict_imps, avg_imps, percent_formatter, plot_comparison_for_season

import joblib
# Load the best model from Models.ipynb
best_model = joblib.load('best_model.pkl')

# Load the data
df_selected = pd.read_csv('df_selected.csv')
features = list(df_selected.columns)
features.append('mvp_share')
features.append('Rank')
df_train = pd.read_csv('df_clean.csv', usecols=features)
labels = df_train.pop("mvp_share")
stratify = df_train.pop("Rank")
del features[10]
del features[10]
features.append('Season')
features.append('name')
df_test = pd.read_csv('df_last.csv', usecols=features)
df_test.rename(columns={'name': 'Name'}, inplace=True)
del features[10]
del features[10]

(X_train, X_test, y_train, y_test) = train_test_split(df_train, 
                                                      labels, 
                                                      test_size=0.2, 
                                                      shuffle=True, 
                                                      random_state=28, 
                                                      stratify=stratify)

# Convert each dataset to array
y_train = y_train.values
y_test = y_test.values
X_train = X_train.values
X_test = X_test.values

best_model.fit(X_train, y_train)

# Make predictions on the test data using the best model
y_pred = best_model.predict(X_test)

# Evaluate the best model using mean squared error and R-squared
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Test MSE:", mse)
print("Test R-squared:", r2)

dfs_n_last = []
for season_n in df_test['Season'].unique():
        df_n = df_test[df_test['Season'] == season_n].copy()
        names_n = df_n["Name"].values
        df_n.drop(['Season', 'Name'], axis="columns", inplace=True)
        feature_n = df_n.to_numpy()

        prediction = best_model.predict(feature_n)
        df_curr = pd.DataFrame(data=feature_n, index=None, columns=features)
        df_curr['Season'] = season_n
        df_curr['name'] = names_n
        df_curr['predicted'] = prediction * 100
        dfs_n_last.append(df_curr)
        df_curr = df_curr.sort_values(by=['predicted'], ascending=False, ignore_index=True)
        
df_pred = pd.concat(dfs_n_last, ignore_index=True)

#df_pred = pd.read_csv('predictions.csv')
keep = list(df_pred.columns)
del keep[12]
keep.append('mvp_share')
df_full = pd.read_csv('mvp_data_edit.csv', usecols=keep)
# Merge df_pred with df_full on "name" and "Season" columns
merged_df = pd.merge(df_pred, df_full[['name', 'Season', 'mvp_share']], 
                     on=['name', 'Season'], how='left')
# Rename the 'mvp_share' column to 'actual' in the merged dataframe
merged_df.rename(columns={'mvp_share': 'actual'}, inplace=True)
merged_df['actual'] *= 100

# Define custom colors for 'predicted' and 'actual'
custom_palette = {'predicted': '#E57200', 'actual': '#232D4B'}

# Iterate over unique values in the 'Season' column and create separate plots for each
unique_seasons = merged_df['Season'].unique()

plot_comparison_for_season(merged_df, 2022)
plot_comparison_for_season(merged_df, 2021)
plot_comparison_for_season(merged_df, 2020)
plot_comparison_for_season(merged_df, 2019)
plot_comparison_for_season(merged_df, 2018)

df_results = pd.read_csv('mvp_data_edit.csv')
# Drop Wins and Conference because we combined in cleaning notebook
df_results.drop(columns=['conference', 'W'], inplace=True)
# Filter to seasons after 1980 as we do in training
df_results = df_results[df_results['Season'] >= 1980]
# Pull out the feature importances
feature_importances = best_model.feature_importances_
# Normalize feature importances
normalized_importances = feature_importances / np.sum(feature_importances)
# Construct index
index_values = np.dot(df_results[features].values, normalized_importances)
# Add index values as a new column to the DataFrame
df_results['index'] = index_values
# Rank the 'Index' column within each season group and store the result in a new column 'Ranked_Index'
df_results['Ranked_Index'] = df_results.groupby('Season')['index'].rank(ascending=False)

df_results.to_csv('results.csv', index=False)
```
</details>
</details>

<details>
<summary><h1 style="font-size: 16px;">Manifest</h1></summary>

<details>
<summary><h3 style="font-size: 14px;">Jupyter Notebooks</h3></summary>
  
- #### [FeatureSelection.ipynb](https://github.com/WD-Scott/DS5110_Project/blob/main/Jupyter%20Notebooks/FeatureSelection.ipynb):

  Feature Selection notebook where we use the `preprocess_and_train` function from `preptrain.py` and ensemble the methods to generate the best 10 features.
  
- #### [DataCleaning_EDA.ipynb](https://github.com/WD-Scott/DS5110_Project/blob/main/Jupyter%20Notebooks/DataCleaning_EDA.ipynb):
  
  Exploratory notebook where the data is cleaned; includes some basic EDA.

- #### [Models.ipynb](https://github.com/WD-Scott/DS5110_Project/blob/main/Jupyter%20Notebooks/Models.ipynb):

  Modeling notebook where we use the selected features (from `df_selected.csv`) to train and evaluate a range of models and extract their feature importance. These results will inform how we weight features in the index.

- #### [Test.ipynb](https://github.com/WD-Scott/DS5110_Project/blob/main/Jupyter%20Notebooks/Test.ipynb):

  This notebook contains the code where we test our best model (from `Models.ipynb`) against the last five seasons. We include some visualizations showing the model prediction versus the actual values.

</details>

<details>
<summary><h3 style="font-size: 14px;">Data Files</h3></summary>
  
- #### [df_clean.csv](https://github.com/WD-Scott/DS5110_Project/blob/main/Data%20Files/df_clean.csv):
  
  Main file used for training and validation.

- #### [df_last.csv](https://github.com/WD-Scott/DS5110_Project/blob/main/Data%20Files/df_last.csv):
  
  Testing file for examining model performance on last 5 seasons (2018-22).

- #### [df_selected.csv](https://github.com/WD-Scott/DS5110_Project/blob/main/Data%20Files/df_selected.csv):

  Selected features containing the subset of predictor variables.

- #### [mvp_data.csv](https://github.com/WD-Scott/DS5110_Project/blob/main/Data%20Files/mvp_data.csv):

  Initial NBA mvp data set. Reduced in [DataCleaning_EDA.ipynb](https://github.com/WD-Scott/DS5110_Project/blob/main/Jupyter%20Notebooks/DataCleaning_EDA.ipynb) to only include essential rows and columns of study.

- #### [mvp_data_edit.csv](https://github.com/WD-Scott/DS5110_Project/blob/main/Data%20Files/mvp_data_edit.csv)

  The cleaned data from [DataCleaning_EDA.ipynb](https://github.com/WD-Scott/DS5110_Project/blob/main/Jupyter%20Notebooks/DataCleaning_EDA.ipynb), used in [Test.ipynb](https://github.com/WD-Scott/DS5110_Project/blob/main/Jupyter%20Notebooks/Test.ipynb) to merge and compare predicted and actual values.

- #### [results.csv](https://github.com/WD-Scott/DS5110_Project/blob/main/Data%20Files/results.csv)

  The full dataset with the index values calculated and stored as an additional column.
  
</details>

<details>
<summary><h3 style="font-size: 14px;">Python Modules (helper functions, classes)</h3></summary>
  
- #### [preptrain.py](https://github.com/WD-Scott/DS5110_Project/blob/main/Python%20Modules/preptrain.py):
  
  Custom function/pipeline for preprocessing and feature selection.

- #### [modeling.py](https://github.com/WD-Scott/DS5110_Project/blob/main/Python%20Modules/modeling.py):

  Custom function/pipeline to train the ensemble and tree-based models and extract the best model.

- #### [helper_functions.py](https://github.com/WD-Scott/DS5110_Project/blob/main/Python%20Modules/helper_functions.py):

This module contains various helper functions for system information retrieval, model evaluation, and visualization.
    
- `get_hardware_details()`:
  
  Retrieve basic hardware details of the system.

- `print_importances(features, model)`:
  
  Print the feature importances of a model.

- `print_dict_imps(feature_importances)`:
  
  Print the feature importances in a visually appealing table format side-by-side for model comparison.

- `print_dict_imps4x2(feature_importances)`:
  
  Print the feature importances in a visually appealing table format side-by-side for feature selection.

- `avg_imps(feature_importances)`:
  
  Calculate the average feature importances across different methods.

- `create_imp_df(model_names, models, feature_names)`:
  
  Create a DataFrame of feature importances for each model.

- `plot_corr_heatmap(corr_matrix, selected_feature_names, threshold=0.65, width=7, height=4)`:
  
  Plot a correlation heatmap for selected features.

- `plot_model_performance(model_names, r_sqs, MSE_s)`:
  
  Plot the R-squared and MSE values of different regression models.

- `plot_comparison_for_season(df, season)`

  Plot the actual vs. predicted mvp_share values.
  
</details>

<details>
<summary><h3 style="font-size: 14px;">Other Files</h3></summary>

- #### [images](https://github.com/WD-Scott/DS5110_Project/tree/main/images):

The images folder contains various visualizations and images used in the README.md

- #### [README.md](https://github.com/WD-Scott/DS5110_Project/blob/main/README.md):

The README.md file includes the repository description and the report.

- #### [requirements.txt](https://github.com/WD-Scott/DS5110_Project/blob/main/requirements.txt):

This file includes all of the necessary libraries and versions for running our code.
</details>
</details>
